#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - Enhanced Module Processor
Processador aprimorado de m√≥dulos com IA
"""

import os
import logging
import asyncio
import json
from typing import Dict, List, Any
from datetime import datetime
from pathlib import Path

# Import do AI Manager
from services.ai_manager import ai_manager
from services.auto_save_manager import salvar_etapa, salvar_erro
# CORRE√á√ÉO 1: Importar a fun√ß√£o com o nome correto
from modules.cpl_creator import create_devastating_cpl_protocol # Import do novo m√≥dulo

logger = logging.getLogger(__name__)

class EnhancedModuleProcessor:
    """Processador aprimorado de m√≥dulos"""

    def __init__(self):
        """Inicializa o processador"""
        self.ai_manager = ai_manager

        # Lista completa dos m√≥dulos (incluindo o novo m√≥dulo CPL)
        self.modules_config = {
            'anti_objecao': {
                'title': 'Sistema Anti-Obje√ß√£o',
                'description': 'Sistema completo para antecipar e neutralizar obje√ß√µes',
                'use_active_search': False,
                'type': 'standard'
            },
            'avatars': {
                'title': 'Avatares do P√∫blico-Alvo',
                'description': 'Personas detalhadas do p√∫blico-alvo',
                'use_active_search': False,
                'type': 'standard'
            },
            'concorrencia': {
                'title': 'An√°lise Competitiva',
                'description': 'An√°lise completa da concorr√™ncia',
                'use_active_search': True,
                'type': 'standard'
            },
            'drivers_mentais': {
                'title': 'Drivers Mentais',
                'description': 'Gatilhos psicol√≥gicos e drivers de compra',
                'use_active_search': False,
                'type': 'standard'
            },
            'funil_vendas': {
                'title': 'Funil de Vendas',
                'description': 'Estrutura completa do funil de vendas',
                'use_active_search': False,
                'type': 'standard'
            },
            'insights_mercado': {
                'title': 'Insights de Mercado',
                'description': 'Insights profundos sobre o mercado',
                'use_active_search': True,
                'type': 'standard'
            },
            'palavras_chave': {
                'title': 'Estrat√©gia de Palavras-Chave',
                'description': 'Estrat√©gia completa de SEO e palavras-chave',
                'use_active_search': False,
                'type': 'standard'
            },
            'plano_acao': {
                'title': 'Plano de A√ß√£o',
                'description': 'Plano de a√ß√£o detalhado e execut√°vel',
                'use_active_search': False,
                'type': 'standard'
            },
            'posicionamento': {
                'title': 'Estrat√©gia de Posicionamento',
                'description': 'Posicionamento estrat√©gico no mercado',
                'use_active_search': False,
                'type': 'standard'
            },
            'pre_pitch': {
                'title': 'Estrutura de Pr√©-Pitch',
                'description': 'Estrutura de pr√©-venda e engajamento',
                'use_active_search': False,
                'type': 'standard'
            },
            'predicoes_futuro': {
                'title': 'Predi√ß√µes de Mercado',
                'description': 'Predi√ß√µes e tend√™ncias futuras',
                'use_active_search': True,
                'type': 'standard'
            },
            'provas_visuais': {
                'title': 'Sistema de Provas Visuais',
                'description': 'Provas visuais e sociais',
                'use_active_search': False,
                'type': 'standard'
            },
            'metricas_conversao': {
                'title': 'M√©tricas de Convers√£o',
                'description': 'KPIs e m√©tricas de convers√£o',
                'use_active_search': False,
                'type': 'standard'
            },
            'estrategia_preco': {
                'title': 'Estrat√©gia de Precifica√ß√£o',
                'description': 'Estrat√©gia de pre√ßos e monetiza√ß√£o',
                'use_active_search': False,
                'type': 'standard'
            },
            'canais_aquisicao': {
                'title': 'Canais de Aquisi√ß√£o',
                'description': 'Canais de aquisi√ß√£o de clientes',
                'use_active_search': False,
                'type': 'standard'
            },
            'cronograma_lancamento': {
                'title': 'Cronograma de Lan√ßamento',
                'description': 'Cronograma detalhado de lan√ßamento',
                'use_active_search': False,
                'type': 'standard'
            },
            'cpl_completo': {
                'title': 'Protocolo Integrado de CPLs Devastadores',
                'description': 'Protocolo completo para cria√ß√£o de sequ√™ncia de 4 CPLs de alta performance',
                'use_active_search': True,
                'type': 'specialized',
                'requires': ['sintese_master', 'avatar_data', 'contexto_estrategico', 'dados_web']
            }
        }

        logger.info("üöÄ Enhanced Module Processor inicializado")

    async def generate_all_modules(self, session_id: str) -> Dict[str, Any]:
        """Gera todos os m√≥dulos (16 padr√£o + 1 especializado CPL)"""
        logger.info(f"üöÄ Iniciando gera√ß√£o de todos os m√≥dulos para sess√£o: {session_id}")

        # Carrega dados base
        base_data = self._load_base_data(session_id)

        results = {
            "session_id": session_id,
            "successful_modules": 0,
            "failed_modules": 0,
            "modules_generated": [],
            "modules_failed": [],
            "total_modules": len(self.modules_config)
        }

        # Cria diret√≥rio de m√≥dulos
        modules_dir = Path(f"analyses_data/{session_id}/modules")
        modules_dir.mkdir(parents=True, exist_ok=True)

        # Gera cada m√≥dulo
        for module_name, config in self.modules_config.items():
            try:
                logger.info(f"üìù Gerando m√≥dulo: {module_name}")

                # Verifica se √© o m√≥dulo especializado CPL
                if module_name == 'cpl_completo':
                    # CORRE√á√ÉO 2: Chamar a fun√ß√£o com o nome correto e argumentos ajustados
                    # Gera o m√≥dulo CPL especializado
                    cpl_content = await create_devastating_cpl_protocol(
                        sintese_master=base_data.get('sintese_master', {}),
                        avatar_data=base_data.get('avatar_data', {}),
                        contexto_estrategico=base_data.get('contexto_estrategico', {}),
                        dados_web=base_data.get('dados_web', {}),
                        session_id=session_id # session_id passado como keyword argument
                    )
                    
                    # Salva conte√∫do do m√≥dulo CPL em formato JSON e Markdown
                    cpl_json_path = modules_dir / f"{module_name}.json"
                    with open(cpl_json_path, 'w', encoding='utf-8') as f:
                        json.dump(cpl_content, f, ensure_ascii=False, indent=2)
                    
                    # Cria vers√£o Markdown do conte√∫do CPL
                    cpl_md_content = self._format_cpl_content_to_markdown(cpl_content)
                    cpl_md_path = modules_dir / f"{module_name}.md"
                    with open(cpl_md_path, 'w', encoding='utf-8') as f:
                        f.write(cpl_md_content)
                else:
                    # CORRE√á√ÉO: Sanitiza dados base ANTES de usar
                    sanitized_base_data = self._force_serialize(base_data)
                    
                    # Gera conte√∫do do m√≥dulo padr√£o com retry mechanism
                    max_retries = 3
                    content = None
                    
                    for attempt in range(max_retries):
                        try:
                            if config.get('use_active_search', False):
                                content = await self.ai_manager.generate_with_tools(
                                    prompt=self._get_module_prompt(module_name, config, sanitized_base_data),
                                    context=sanitized_base_data.get('context', ''),
                                    tools=["google_search"],
                                    max_iterations=3
                                )
                            else:
                                content = await self.ai_manager.generate_text(
                                    prompt=self._get_module_prompt(module_name, config, sanitized_base_data)
                                )
                            
                            if content and content.strip():
                                break
                            else:
                                logger.warning(f"‚ö†Ô∏è Conte√∫do vazio na tentativa {attempt + 1} para {module_name}")
                                
                        except Exception as retry_error:
                            error_msg = str(retry_error)
                            if "object" in error_msg.lower() and attempt < max_retries - 1:
                                logger.warning(f"‚ö†Ô∏è Erro 'object' na tentativa {attempt + 1} para {module_name}, tentando novamente...")
                                await asyncio.sleep(2)
                                continue
                            else:
                                raise retry_error
                    
                    if not content or not content.strip():
                        raise Exception(f"Falha ao gerar conte√∫do ap√≥s {max_retries} tentativas")

                    # Salva m√≥dulo padr√£o
                    module_path = modules_dir / f"{module_name}.md"
                    with open(module_path, 'w', encoding='utf-8') as f:
                        f.write(content)

                results["successful_modules"] += 1
                results["modules_generated"].append(module_name)

                logger.info(f"‚úÖ M√≥dulo {module_name} gerado com sucesso")

            except Exception as e:
                error_msg = str(e)
                logger.error(f"‚ùå Erro ao gerar m√≥dulo {module_name}: {error_msg}")
                
                # Diagn√≥stico detalhado do erro
                if "object" in error_msg.lower():
                    logger.error(f"‚ùå ERRO DE SERIALIZA√á√ÉO detectado no m√≥dulo {module_name}")
                    logger.error(f"‚ùå Tipo do erro: {type(e).__name__}")
                    logger.error(f"‚ùå Detalhes: {error_msg}")
                    
                    # Verifica se √© problema com dados base
                    try:
                        json.dumps(base_data, default=str)
                        logger.info("‚úÖ base_data √© serializ√°vel")
                    except Exception as json_err:
                        logger.error(f"‚ùå base_data N√ÉO √© serializ√°vel: {json_err}")
                
                salvar_erro(f"modulo_{module_name}", e, contexto={"session_id": session_id})
                results["failed_modules"] += 1
                results["modules_failed"].append({
                    "module": module_name,
                    "error": error_msg,
                    "error_type": type(e).__name__
                })

        # Gera relat√≥rio consolidado
        await self._generate_consolidated_report(session_id, results)

        logger.info(f"‚úÖ Gera√ß√£o conclu√≠da: {results['successful_modules']}/{results['total_modules']} m√≥dulos")

        return results

    def _load_base_data(self, session_id: str) -> Dict[str, Any]:
        """Carrega dados base da sess√£o"""
        try:
            session_dir = Path(f"analyses_data/{session_id}")

            # Carrega s√≠nteses
            synthesis_data = {}
            for synthesis_file in session_dir.glob("sintese_*.json"):
                try:
                    with open(synthesis_file, 'r', encoding='utf-8') as f:
                        synthesis_data[synthesis_file.stem] = json.load(f)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao carregar s√≠ntese {synthesis_file}: {e}")

            # Carrega relat√≥rio de coleta
            coleta_content = ""
            coleta_file = session_dir / "relatorio_coleta.md"
            if coleta_file.exists():
                with open(coleta_file, 'r', encoding='utf-8') as f:
                    coleta_content = f.read()

            # Carrega dados espec√≠ficos para o m√≥dulo CPL
            sintese_master = {}
            avatar_data = {}
            contexto_estrategico = {}
            dados_web = {}
            
            # Tenta carregar a s√≠ntese master (m√∫ltiplos formatos poss√≠veis)
            sintese_master_files = [
                session_dir / "sintese_master_synthesis.json",
                session_dir / "sintese_master.json",
                session_dir / "sintese_master_data.json"
            ]
            
            for sintese_master_file in sintese_master_files:
                if sintese_master_file.exists():
                    try:
                        with open(sintese_master_file, 'r', encoding='utf-8') as f:
                            sintese_master = json.load(f)
                        logger.info(f"‚úÖ S√≠ntese master carregada de: {sintese_master_file.name}")
                        break
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Erro ao carregar s√≠ntese master de {sintese_master_file.name}: {e}")
            
            # Tenta carregar dados do avatar
            avatar_file = session_dir / "avatar_detalhado.json"
            if avatar_file.exists():
                try:
                    with open(avatar_file, 'r', encoding='utf-8') as f:
                        avatar_data = json.load(f)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao carregar dados do avatar: {e}")
            
            # Tenta carregar contexto estrat√©gico
            contexto_file = session_dir / "contexto_estrategico.json"
            if contexto_file.exists():
                try:
                    with open(contexto_file, 'r', encoding='utf-8') as f:
                        contexto_estrategico = json.load(f)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao carregar contexto estrat√©gico: {e}")
            
            # Tenta carregar dados da web
            web_data_file = session_dir / "dados_pesquisa_web.json"
            if web_data_file.exists():
                try:
                    with open(web_data_file, 'r', encoding='utf-8') as f:
                        dados_web = json.load(f)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao carregar dados da web: {e}")

            # Sanitiza dados para garantir serializa√ß√£o
            base_data = {
                "synthesis_data": self._sanitize_data(synthesis_data),
                "coleta_content": str(coleta_content),
                "context": f"Dados de s√≠ntese: {len(synthesis_data)} arquivos. Relat√≥rio de coleta: {len(coleta_content)} caracteres.",
                "sintese_master": self._sanitize_data(sintese_master),
                "avatar_data": self._sanitize_data(avatar_data),
                "contexto_estrategico": self._sanitize_data(contexto_estrategico),
                "dados_web": self._sanitize_data(dados_web)
            }
            
            # Testa serializa√ß√£o
            try:
                json.dumps(base_data, default=str)
                logger.debug("‚úÖ base_data √© serializ√°vel")
            except Exception as e:
                logger.error(f"‚ùå base_data n√£o √© serializ√°vel: {e}")
                # For√ßa convers√£o para string de todos os valores problem√°ticos
                base_data = self._force_serialize(base_data)
            
            return base_data

        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar dados base: {e}")
            return {
                "synthesis_data": {}, 
                "coleta_content": "", 
                "context": "",
                "sintese_master": {},
                "avatar_data": {},
                "contexto_estrategico": {},
                "dados_web": {}
            }

    def _sanitize_data(self, data: Any) -> Any:
        """Sanitiza dados para garantir serializa√ß√£o JSON"""
        if data is None:
            return None
        elif isinstance(data, (str, int, float, bool)):
            return data
        elif isinstance(data, dict):
            return {str(k): self._sanitize_data(v) for k, v in data.items()}
        elif isinstance(data, (list, tuple)):
            return [self._sanitize_data(item) for item in data]
        else:
            # Converte objetos complexos para string
            return str(data)

    def _force_serialize(self, data: Any) -> Any:
        """For√ßa serializa√ß√£o convertendo tudo para tipos b√°sicos"""
        try:
            return json.loads(json.dumps(data, default=str))
        except Exception as e:
            logger.error(f"‚ùå Erro na serializa√ß√£o for√ßada: {e}")
            return str(data)

    def _get_module_prompt(self, module_name: str, config: Dict[str, Any], base_data: Dict[str, Any]) -> str:
        """Gera prompt para um m√≥dulo espec√≠fico"""
        
        # CORRE√á√ÉO: Garante que sempre h√° contexto suficiente
        context = base_data.get('context', 'An√°lise de mercado digital brasileiro')
        coleta_content = base_data.get('coleta_content', 'Dados de mercado digital e tend√™ncias atuais')
        
        # Se n√£o h√° dados espec√≠ficos, usa contexto gen√©rico mas √∫til
        if not context or context == 'Dados limitados':
            context = f"An√°lise de {config['description'].lower()} no mercado brasileiro atual"
        
        if not coleta_content:
            coleta_content = f"Tend√™ncias atuais em {config['description'].lower()} e melhores pr√°ticas do mercado"

        base_prompt = f"""# {config['title']}

Voc√™ √© um especialista em {config['description'].lower()} com vasta experi√™ncia no mercado brasileiro.

## CONTEXTO DA AN√ÅLISE:
{context}

## TAREFA:
Crie um m√≥dulo ultra-detalhado sobre {config['title']} com base nas melhores pr√°ticas e tend√™ncias atuais.

## ESTRUTURA OBRIGAT√ìRIA:
1. **Resumo Executivo** (200-300 palavras)
2. **An√°lise Detalhada** (500-700 palavras)
3. **Estrat√©gias Espec√≠ficas** (400-600 palavras)
4. **Implementa√ß√£o Pr√°tica** (300-500 palavras)
5. **M√©tricas e KPIs** (200-300 palavras)
6. **Cronograma de Execu√ß√£o** (200-300 palavras)

## REQUISITOS OBRIGAT√ìRIOS:
- M√çNIMO 2000 palavras no total
- Dados espec√≠ficos do mercado brasileiro
- Estrat√©gias acion√°veis e pr√°ticas
- M√©tricas mensur√°veis e realistas
- Formato markdown profissional
- Exemplos concretos e casos de uso

## DADOS DE REFER√äNCIA:
{coleta_content[:1000]}

## IMPORTANTE:
- SEMPRE gere conte√∫do completo e detalhado
- Use dados reais do mercado brasileiro quando poss√≠vel
- Inclua exemplos pr√°ticos e acion√°veis
- Mantenha tom profissional e consultivo

COMECE AGORA a gerar o m√≥dulo completo:
"""

        return base_prompt

    def _format_cpl_content_to_markdown(self, cpl_content: Dict[str, Any]) -> str:
        """Formata o conte√∫do do m√≥dulo CPL para Markdown"""
        try:
            markdown_content = f"""# {cpl_content.get('titulo', 'Protocolo de CPLs Devastadores')}

{cpl_content.get('descricao', '')}

"""

            # Adiciona cada fase do protocolo
            fases = cpl_content.get('fases', {})
            for fase_key, fase_data in fases.items():
                markdown_content += f"## {fase_data.get('titulo', fase_key)}\n\n"
                markdown_content += f"**{fase_data.get('descricao', '')}**\n\n"
                
                # Adiciona se√ß√µes espec√≠ficas de cada fase
                if 'estrategia' in fase_data:
                    markdown_content += f"### Estrat√©gia\n{fase_data['estrategia']}\n\n"
                
                if 'versoes_evento' in fase_data:
                    markdown_content += "### Vers√µes do Evento\n"
                    for versao in fase_data['versoes_evento']:
                        markdown_content += f"- **{versao.get('nome_evento', '')}** ({versao.get('tipo', '')}): {versao.get('justificativa_psicologica', '')}\n"
                    markdown_content += "\n"
                
                if 'teasers' in fase_data:
                    markdown_content += "### Teasers\n"
                    for teaser in fase_data['teasers']:
                        markdown_content += f"- {teaser.get('texto', '')} (*{teaser.get('justificativa', '')}*)\n"
                    markdown_content += "\n"
                
                if 'historia_transformacao' in fase_data:
                    ht = fase_data['historia_transformacao']
                    markdown_content += "### Hist√≥ria de Transforma√ß√£o\n"
                    markdown_content += f"- **Antes**: {ht.get('antes', '')}\n"
                    markdown_content += f"- **Durante**: {ht.get('durante', '')}\n"
                    markdown_content += f"- **Depois**: {ht.get('depois', '')}\n\n"
                
                # Adiciona outras se√ß√µes conforme necess√°rio...
                markdown_content += "---\n\n"
            
            # Adiciona considera√ß√µes finais
            consideracoes = cpl_content.get('consideracoes_finais', {})
            if consideracoes:
                markdown_content += "## Considera√ß√µes Finais\n\n"
                markdown_content += f"**Impacto Previsto**: {consideracoes.get('impacto_previsto', '')}\n\n"
                
                if consideracoes.get('diferenciais'):
                    markdown_content += "### Diferenciais\n"
                    for diferencial in consideracoes['diferenciais']:
                        markdown_content += f"- {diferencial}\n"
                    markdown_content += "\n"
                
                if consideracoes.get('proximos_passos'):
                    markdown_content += "### Pr√≥ximos Passos\n"
                    for passo in consideracoes['proximos_passos']:
                        markdown_content += f"- {passo}\n"
                    markdown_content += "\n"

            return markdown_content
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao formatar conte√∫do CPL para Markdown: {e}")
            return "# Protocolo de CPLs Devastadores\n\n*Erro ao gerar conte√∫do formatado*"

    async def _generate_consolidated_report(self, session_id: str, results: Dict[str, Any]) -> None:
        """Gera relat√≥rio consolidado final"""
        try:
            logger.info("üìã Gerando relat√≥rio consolidado final...")

            # Carrega todos os m√≥dulos gerados
            modules_dir = Path(f"analyses_data/{session_id}/modules")
            consolidated_content = f"""# RELAT√ìRIO FINAL CONSOLIDADO - ARQV30 Enhanced v3.0

**Sess√£o:** {session_id}  
**Data:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}  
**M√≥dulos Gerados:** {results['successful_modules']}/{results['total_modules']}  
**Taxa de Sucesso:** {(results['successful_modules']/results['total_modules']*100):.1f}%

---

## SUM√ÅRIO EXECUTIVO

Este relat√≥rio consolida {results['successful_modules']} m√≥dulos especializados de an√°lise estrat√©gica gerados pelo sistema ARQV30 Enhanced v3.0.

## M√ìDULOS INCLU√çDOS

"""

            # Adiciona cada m√≥dulo gerado (incluindo o novo CPL)
            for module_name in results['modules_generated']:
                # Trata o m√≥dulo CPL de forma especial
                if module_name == 'cpl_completo':
                    cpl_json_file = modules_dir / f"{module_name}.json"
                    if cpl_json_file.exists():
                        try:
                            with open(cpl_json_file, 'r', encoding='utf-8') as f:
                                cpl_data = json.load(f)
                                title = cpl_data.get('titulo', self.modules_config[module_name]['title'])
                                descricao = cpl_data.get('descricao', '')
                                consolidated_content += f"\n## {title}\n\n{descricao}\n\n"
                                
                                # Adiciona um resumo das fases
                                fases = cpl_data.get('fases', {})
                                if fases:
                                    consolidated_content += "### Fases do Protocolo:\n"
                                    for fase_key, fase_data in fases.items():
                                        consolidated_content += f"- **{fase_data.get('titulo', fase_key)}**: {fase_data.get('descricao', '')[:100]}...\n"
                                    consolidated_content += "\n"
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Erro ao carregar conte√∫do CPL para relat√≥rio: {e}")
                            consolidated_content += f"\n## {self.modules_config[module_name]['title']}\n\n*Conte√∫do n√£o dispon√≠vel*\n\n"
                    else:
                        consolidated_content += f"\n## {self.modules_config[module_name]['title']}\n\n*Conte√∫do n√£o gerado*\n\n"
                else:
                    # Trata m√≥dulos padr√£o
                    module_file = modules_dir / f"{module_name}.md"
                    if module_file.exists():
                        try:
                            with open(module_file, 'r', encoding='utf-8') as f:
                                content = f.read()
                                title = self.modules_config[module_name]['title']
                                # Extrai apenas o t√≠tulo e resumo executivo para o relat√≥rio consolidado
                                lines = content.split('\n')
                                summary_lines = []
                                in_executive_summary = False
                                
                                for line in lines:
                                    if line.startswith('# ') and 'Resumo Executivo' in line:
                                        in_executive_summary = True
                                        summary_lines.append(line)
                                    elif in_executive_summary and line.startswith('#') and 'Resumo Executivo' not in line:
                                        break
                                    elif in_executive_summary:
                                        summary_lines.append(line)
                                
                                if summary_lines:
                                    consolidated_content += f"\n## {title}\n\n" + '\n'.join(summary_lines[1:10]) + "\n\n"
                                else:
                                    # Se n√£o encontrar resumo executivo, usa as primeiras linhas
                                    consolidated_content += f"\n## {title}\n\n" + '\n'.join(lines[:5]) + "\n\n"
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Erro ao carregar conte√∫do do m√≥dulo {module_name} para relat√≥rio: {e}")
                            consolidated_content += f"\n## {self.modules_config[module_name]['title']}\n\n*Conte√∫do n√£o dispon√≠vel*\n\n"
                consolidated_content += "---\n\n"

            # Adiciona informa√ß√µes de m√≥dulos falhados
            if results['modules_failed']:
                consolidated_content += "\n## M√ìDULOS N√ÉO GERADOS\n\n"
                for failed in results['modules_failed']:
                    consolidated_content += f"- **{failed['module']}**: {failed['error']}\n"

            # Salva relat√≥rio consolidado
            consolidated_path = f"analyses_data/{session_id}/relatorio_final_completo.md"
            with open(consolidated_path, 'w', encoding='utf-8') as f:
                f.write(consolidated_content)

            logger.info(f"‚úÖ Relat√≥rio consolidado salvo em: {consolidated_path}")

        except Exception as e:
            logger.error(f"‚ùå Erro ao gerar relat√≥rio consolidado: {e}")
            salvar_erro("relatorio_consolidado", e, contexto={"session_id": session_id})

# Inst√¢ncia global
enhanced_module_processor = EnhancedModuleProcessor()
