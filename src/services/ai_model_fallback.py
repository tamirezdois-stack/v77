#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - AI Model Fallback System
Sistema de fallback para modelos de AI: OpenRouter (Qwen) ‚Üí Gemini ‚Üí Groq
"""

import os
import logging
import asyncio
from typing import Dict, List, Any, Optional, Union
import json

from .api_rotation_manager import api_rotation_manager

logger = logging.getLogger(__name__)

class AIModelFallback:
    """Sistema de fallback para modelos de AI"""
    
    def __init__(self):
        """Inicializa o sistema de fallback"""
        self.model_configs = {
            'OPENROUTER': {
                'models': ['qwen-2.5-72b-instruct', 'qwen-2.5-32b-instruct'],
                'base_url': 'https://openrouter.ai/api/v1',
                'timeout': 60
            },
            'GEMINI': {
                'models': ['gemini-2.0-flash-exp', 'gemini-1.5-pro'],
                'base_url': 'https://generativelanguage.googleapis.com/v1beta',
                'timeout': 45
            },
            'GROQ': {
                'models': ['llama-3.1-70b-versatile', 'mixtral-8x7b-32768'],
                'base_url': 'https://api.groq.com/openai/v1',
                'timeout': 30
            }
        }
        
        logger.info("ü§ñ AI Model Fallback System inicializado")
    
    async def generate_completion(
        self,
        prompt: str,
        max_tokens: int = 4000,
        temperature: float = 0.7,
        system_prompt: str = None
    ) -> Dict[str, Any]:
        """Gera completion com fallback autom√°tico entre modelos"""
        
        # Ordem de prioridade dos provedores
        providers = ['OPENROUTER', 'GEMINI', 'GROQ']
        
        for provider in providers:
            try:
                logger.info(f"ü§ñ Tentando {provider} para gera√ß√£o de texto")
                
                result = await self._generate_with_provider(
                    provider, prompt, max_tokens, temperature, system_prompt
                )
                
                if result.get('success'):
                    logger.info(f"‚úÖ {provider} gerou {len(result.get('content', ''))} caracteres")
                    api_rotation_manager.report_success(provider)
                    return result
                else:
                    logger.warning(f"‚ö†Ô∏è {provider} falhou: {result.get('error', 'Erro desconhecido')}")
                    api_rotation_manager.report_failure(provider, result.get('error', ''))
                    
            except Exception as e:
                logger.error(f"‚ùå Erro em {provider}: {e}")
                api_rotation_manager.report_failure(provider, str(e))
                continue
        
        # Se todos falharam
        return {
            'success': False,
            'error': 'Todos os modelos de AI falharam',
            'content': '',
            'provider_used': 'none'
        }
    
    async def _generate_with_provider(
        self,
        provider: str,
        prompt: str,
        max_tokens: int,
        temperature: float,
        system_prompt: str = None
    ) -> Dict[str, Any]:
        """Gera texto com um provedor espec√≠fico"""
        
        if provider == 'OPENROUTER':
            return await self._generate_with_openrouter(prompt, max_tokens, temperature, system_prompt)
        elif provider == 'GEMINI':
            return await self._generate_with_gemini(prompt, max_tokens, temperature, system_prompt)
        elif provider == 'GROQ':
            return await self._generate_with_groq(prompt, max_tokens, temperature, system_prompt)
        else:
            raise Exception(f"Provedor {provider} n√£o suportado")
    
    async def _generate_with_openrouter(
        self,
        prompt: str,
        max_tokens: int,
        temperature: float,
        system_prompt: str = None
    ) -> Dict[str, Any]:
        """Gera texto usando OpenRouter"""
        import aiohttp
        
        api_key = api_rotation_manager.get_api_key('OPENROUTER')
        if not api_key:
            raise Exception("Nenhuma chave OpenRouter dispon√≠vel")
        
        headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json',
            'HTTP-Referer': 'https://arqv30.com',
            'X-Title': 'ARQV30 Enhanced'
        }
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        payload = {
            'model': 'qwen/qwen-2.5-72b-instruct',
            'messages': messages,
            'max_tokens': max_tokens,
            'temperature': temperature,
            'stream': False
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                'https://openrouter.ai/api/v1/chat/completions',
                headers=headers,
                json=payload,
                timeout=60
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    if data.get('choices') and len(data['choices']) > 0:
                        content = data['choices'][0]['message']['content']
                        
                        return {
                            'success': True,
                            'content': content,
                            'provider_used': 'openrouter',
                            'model_used': 'qwen-2.5-72b-instruct',
                            'tokens_used': data.get('usage', {}).get('total_tokens', 0)
                        }
                    else:
                        raise Exception("OpenRouter n√£o retornou choices v√°lidas")
                else:
                    error_text = await response.text()
                    raise Exception(f"OpenRouter retornou status {response.status}: {error_text}")
    
    async def _generate_with_gemini(
        self,
        prompt: str,
        max_tokens: int,
        temperature: float,
        system_prompt: str = None
    ) -> Dict[str, Any]:
        """Gera texto usando Gemini"""
        import aiohttp
        
        api_key = api_rotation_manager.get_api_key('GEMINI')
        if not api_key:
            raise Exception("Nenhuma chave Gemini dispon√≠vel")
        
        # Combina system prompt com user prompt se necess√°rio
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"{system_prompt}\n\n{prompt}"
        
        payload = {
            'contents': [{
                'parts': [{'text': full_prompt}]
            }],
            'generationConfig': {
                'maxOutputTokens': max_tokens,
                'temperature': temperature
            }
        }
        
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key={api_key}"
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                url,
                json=payload,
                timeout=45
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    if data.get('candidates') and len(data['candidates']) > 0:
                        candidate = data['candidates'][0]
                        if candidate.get('content') and candidate['content'].get('parts'):
                            content = candidate['content']['parts'][0]['text']
                            
                            return {
                                'success': True,
                                'content': content,
                                'provider_used': 'gemini',
                                'model_used': 'gemini-2.0-flash-exp',
                                'tokens_used': data.get('usageMetadata', {}).get('totalTokenCount', 0)
                            }
                        else:
                            raise Exception("Gemini n√£o retornou conte√∫do v√°lido")
                    else:
                        raise Exception("Gemini n√£o retornou candidates v√°lidos")
                else:
                    error_text = await response.text()
                    raise Exception(f"Gemini retornou status {response.status}: {error_text}")
    
    async def _generate_with_groq(
        self,
        prompt: str,
        max_tokens: int,
        temperature: float,
        system_prompt: str = None
    ) -> Dict[str, Any]:
        """Gera texto usando Groq"""
        import aiohttp
        
        api_key = api_rotation_manager.get_api_key('GROQ')
        if not api_key:
            raise Exception("Nenhuma chave Groq dispon√≠vel")
        
        headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        payload = {
            'model': 'llama-3.1-70b-versatile',
            'messages': messages,
            'max_tokens': max_tokens,
            'temperature': temperature,
            'stream': False
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                'https://api.groq.com/openai/v1/chat/completions',
                headers=headers,
                json=payload,
                timeout=30
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    if data.get('choices') and len(data['choices']) > 0:
                        content = data['choices'][0]['message']['content']
                        
                        return {
                            'success': True,
                            'content': content,
                            'provider_used': 'groq',
                            'model_used': 'llama-3.1-70b-versatile',
                            'tokens_used': data.get('usage', {}).get('total_tokens', 0)
                        }
                    else:
                        raise Exception("Groq n√£o retornou choices v√°lidas")
                else:
                    error_text = await response.text()
                    raise Exception(f"Groq retornou status {response.status}: {error_text}")
    
    async def analyze_content(
        self,
        content: str,
        analysis_type: str = "general",
        context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """Analisa conte√∫do com fallback autom√°tico"""
        
        # Prompts espec√≠ficos por tipo de an√°lise
        analysis_prompts = {
            "general": "Analise o seguinte conte√∫do e forne√ßa insights detalhados:",
            "viral": "Analise este conte√∫do do ponto de vista de viralidade e engajamento:",
            "market": "Fa√ßa uma an√°lise de mercado baseada no seguinte conte√∫do:",
            "competitor": "Analise este conte√∫do como intelig√™ncia competitiva:",
            "trend": "Identifique tend√™ncias e padr√µes no seguinte conte√∫do:"
        }
        
        system_prompt = f"""Voc√™ √© um analista especializado em {analysis_type}. 
        Forne√ßa uma an√°lise detalhada, estruturada e acion√°vel.
        Foque em insights pr√°ticos e recomenda√ß√µes espec√≠ficas."""
        
        user_prompt = f"{analysis_prompts.get(analysis_type, analysis_prompts['general'])}\n\n{content}"
        
        if context:
            user_prompt += f"\n\nContexto adicional: {json.dumps(context, ensure_ascii=False)}"
        
        return await self.generate_completion(
            prompt=user_prompt,
            system_prompt=system_prompt,
            max_tokens=3000,
            temperature=0.7
        )
    
    async def summarize_content(
        self,
        content: str,
        max_length: int = 500,
        focus: str = "key_points"
    ) -> Dict[str, Any]:
        """Sumariza conte√∫do com fallback autom√°tico"""
        
        focus_prompts = {
            "key_points": "Extraia os pontos-chave mais importantes",
            "actionable": "Foque em insights acion√°veis e recomenda√ß√µes",
            "trends": "Destaque tend√™ncias e padr√µes identificados",
            "competitive": "Foque em intelig√™ncia competitiva e oportunidades"
        }
        
        system_prompt = f"""Voc√™ √© um especialista em s√≠ntese de informa√ß√µes.
        Crie um resumo conciso e estruturado em at√© {max_length} caracteres.
        {focus_prompts.get(focus, focus_prompts['key_points'])}."""
        
        user_prompt = f"Resuma o seguinte conte√∫do:\n\n{content}"
        
        return await self.generate_completion(
            prompt=user_prompt,
            system_prompt=system_prompt,
            max_tokens=max_length // 2,  # Aproximadamente metade dos tokens para caracteres
            temperature=0.5
        )
    
    def get_status(self) -> Dict[str, Any]:
        """Obt√©m status dos modelos de AI"""
        status = {}
        
        for provider in ['OPENROUTER', 'GEMINI', 'GROQ']:
            provider_status = api_rotation_manager.get_provider_status().get(provider, {})
            status[provider] = {
                'available': provider in api_rotation_manager.providers,
                'healthy': provider_status.get('healthy', False),
                'models': self.model_configs.get(provider, {}).get('models', []),
                'failures': provider_status.get('failures', 0)
            }
        
        return status

# Inst√¢ncia global
ai_model_fallback = AIModelFallback()